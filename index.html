<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ishan Ahluwalia</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="container">
        <nav>
            <a href="#home">Home</a>
            <a href="#research">Research</a>
            <a href="#projects">Projects</a>
            <a href="#experience">Experience</a>
        </nav>

        <section id="home">
            <h1>Ishan Ahluwalia</h1>
            <p class="subtitle">AI & Applied Mathematics | University of Pennsylvania</p>
            <p class="bio">I develop intelligent systems that bridge robotics and artificial intelligence. My work focuses on brain-computer interfaces, reinforcement learning for robotic control, and movement prediction systems for assistive technologies.</p>
            
            <div class="contact">
                <p>ishanah@seas.upenn.edu</p>
                <p><a href="https://linkedin.com/in/ishan-ahluwalia">LinkedIn</a> | <a href="https://github.com/IshanAhluwalia">GitHub</a></p>
            </div>
        </section>

        <section id="research">
            <h2>Research</h2>
            
            <div class="research-item">
                <h3>SUNG Robotics Lab, GRASP Lab, University of Pennsylvania</h3>
                <p class="position">Robotics Researcher | Aug 2025 – Present</p>
                <p>Working on underwater locomotion and distributed robotic systems within Professor Sung's lab, which focuses on embedding intelligence throughout a robot's physical body. Developing reinforcement learning control algorithms for aquatic robots using reward states and Markov Decision Processes to improve maneuverability and robustness. Installing DenseTack Mini 2 tactile sensors for enhanced underwater perception and developing CNN architectures for real-time companion robot location mapping in aquatic environments. Research contributes to the lab's mission of creating more adaptive and efficient robotic platforms through computational co-design approaches.</p>
                <p><strong>Lab Website:</strong> <a href="https://sung.seas.upenn.edu/" target="_blank">sung.seas.upenn.edu</a></p>
            </div>

            <div class="research-item">
                <h3>LDOS Lab, University of Pennsylvania</h3>
                <p class="position">OS/AI Researcher | Jan 2025 – Sep 2025</p>
                <p>Contributing to the Learning Directed Operating System project, which aims to replace traditional manual OS heuristics with intelligent machine learning-based resource management. Profiled allocative slow-path bottlenecks across page compaction routines using kyprobes and custom benchmarking for 127K+ lines of Linux kernel (mm/compaction.c). Designed and implemented recursive data management algorithms to optimize memory fragmentation, achieving ~18% improvement in memory throughput on synthetic workloads. Research supports LDOS's mission to create self-adaptive operating systems that can dynamically optimize performance for complex real-time applications, autonomous systems, and edge computing environments.</p>
                <p><strong>Lab Website:</strong> <a href="https://ldos.utexas.edu/about" target="_blank">ldos.utexas.edu</a></p>
            </div>

        </section>

        <section id="projects">
            <h2>Projects</h2>

            <div class="project featured horizontal">
                <div class="project-content">
                    <h3>NeuroMotus: Brain-Computer Interface for Cerebral Palsy</h3>
                    <p class="position">Independent Research | 2020 – 2024</p>
                    <p>Developed a novel intent-based movement prediction system using EEG signals to optimize exoskeletons for Cerebral Palsy patients. Built CNN architecture achieving 94% accuracy in detecting movement intent from Bereitschaftspotential signals. System predicts 6 essential movements with 87% accuracy, expanding exoskeleton capabilities beyond basic walking patterns.</p>
                    <p><strong>Recognition:</strong> 2nd Place Grand Award & CIA Award, ISEF 2023</p>
                </div>
                
                <div class="project-materials">
                    <div class="media-container">
                        <div class="project-image">
                            <img src="neuromotus-cap.png" alt="NeuroMotus EEG Cap System" />
                            <p class="image-caption">Custom EEG cap with OpenBCI Cyton board</p>
                        </div>
                        
                        <div class="project-video">
                            <video controls width="300">
                                <source src="neuromotus-demo.mp4" type="video/mp4">
                                Your browser does not support the video tag.
                            </video>
                            <p class="image-caption">Live Demo - NWSE 2023</p>
                        </div>
                    </div>
                    
                    <div class="project-links">
                        <a href="neuromotus-poster.pdf">Research Poster</a>
                        <a href="neuromotus-paper.pdf">Research Paper</a>
                        <a href="neuromotus-demo.mp4">Download Demo</a>
                    </div>
                </div>
            </div>

            <div class="project featured horizontal">
                <div class="project-content">
                    <h3>RoboSuite Door Opening</h3>
                    <p class="position">Reinforcement Learning Project | May 2025 – Present</p>
                    <p>Built DDPG actor-critic reinforcement learning system for robotic door manipulation using PyTorch and MuJoCo simulation environment. Implemented tanh actor network (256→128 layers) and Q-critic with experience replay buffer, Gaussian noise exploration, and soft target updates (τ=0.005).</p>
                    <p><strong>Technical Details:</strong> Used Adam/AdamW optimizers with learning rate 1e-3 and custom reward shaping to guide the Panda robot arm through complex door opening sequences. System learns to handle door handle grasping, turning, and pulling motions.</p>
                    <p><strong>Results:</strong> Achieved 98% success rate with mean episodic return improving from 90 at initialization to 230 by 7k training steps under fixed seeds and identical evaluation protocol.</p>
                </div>
                
                <div class="project-materials">
                    <div class="media-container">
                        <div class="project-image">
                            <img src="robosuite-learning-curve.png" alt="RoboSuite Door Opening Learning Curve" />
                            <p class="image-caption">Training Progress: Episodic Return vs Training Steps</p>
                        </div>
                        
                        <div class="project-video">
                            <video controls width="300">
                                <source src="robosuite-door.mp4" type="video/mp4">
                                Your browser does not support the video tag.
                            </video>
                            <p class="image-caption">DDPG Agent Learning Door Opening Task</p>
                        </div>
                    </div>
                    
                    <div class="project-links">
                        <a href="https://github.com/IshanAhluwalia/RLforRoboticDoor" target="_blank">GitHub Code</a>
                        <a href="robosuite-door.mp4">Download Demo</a>
                    </div>
                </div>
            </div>

            <div class="project">
                <h3>Hydroplaning Prevention System</h3>
                <p>Engineered collision prevention method using autoencoder networks paired with tactile vibration sensors, achieving 87% accuracy rate.</p>
                <p><strong>Recognition:</strong> 2nd Place Grand Award, ISEF 2021</p>
            </div>
        </section>

        <section id="experience">
            <h2>Experience</h2>

            <div class="experience-item">
                <h3>MediVoice AI</h3>
                <p class="position">Software Engineering Intern | Jan 2025 - Sep 2025</p>
                <p>Built scalable web scraping pipeline generating 100K+ medical QA pairs using Python and BeautifulSoup. Integrated real-time ASR pipelines using Whisper + VOSK for patient-doctor speech transcription.</p>
            </div>

            <div class="experience-item">
                <h3>Oregon Health & Science University</h3>
                <p class="position">Robotics Systems Engineer Intern | Apr 2023 – Aug 2024</p>
                <p>Built custom CNN architecture for decoding Bereitschaftspotential EEG signals, achieving 23.3% faster response time for virtual hand simulations. Engineered bidirectional neurofeedback loop between EEG input and ROS-controlled limb simulator.</p>
            </div>

            <div class="experience-item">
                <h3>Biomotum Inc</h3>
                <p class="position">Software & Mechanical Engineering Intern | Dec 2021 – Jun 2023</p>
                <p>Developed Python-based calibration algorithms for exoskeletal prosthetics using SVD and PCA. Proposed CAD modifications to Spark prosthetic system improving energy efficiency and wearability.</p>
            </div>
        </section>

        <footer>
            <p>Last updated: October 2024</p>
        </footer>
    </div>
</body>
</html>